# üìÇ utils/ingest.py

# Modules standards
import os
import tempfile  # Pour stocker temporairement les fichiers upload√©s

# Loaders de documents selon leur format
from langchain.document_loaders import PyPDFLoader, TextLoader, UnstructuredWordDocumentLoader

# Pour d√©couper les documents en morceaux exploitables
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Embedding via OpenAI (conversion texte ‚Üí vecteur)
from langchain.embeddings import OpenAIEmbeddings

# Stockage vectoriel (indexation et recherche par similarit√©)
from langchain.vectorstores import FAISS


def process_documents(uploaded_files, api_key):
    """
    Pipeline de traitement des documents :
    1. Sauvegarde temporaire
    2. Chargement (PDF, TXT, DOCX)
    3. D√©coupage en chunks
    4. Embedding OpenAI
    5. Stockage dans FAISS (vectorstore)
    
    Param√®tres :
    - uploaded_files : liste de fichiers upload√©s via Streamlit
    - api_key : cl√© API OpenAI pour l'embedding

    Retour :
    - vectordb : base FAISS contenant les vecteurs pr√™ts pour la recherche
    """

    documents = []  # Stocke tous les documents charg√©s

    for file in uploaded_files:
        # Extension du fichier (.pdf, .txt, .docx)
        suffix = os.path.splitext(file.name)[1]

        # Cr√©ation d‚Äôun fichier temporaire pour le traitement
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
            tmp.write(file.read())        # √âcriture du contenu dans le fichier temporaire
            tmp_path = tmp.name           # Chemin du fichier temporaire

        # S√©lection du bon loader selon le type de fichier
        if suffix == ".pdf":
            loader = PyPDFLoader(tmp_path)
        elif suffix == ".docx":
            loader = UnstructuredWordDocumentLoader(tmp_path)
        else:
            loader = TextLoader(tmp_path)  # Pour les fichiers .txt

        # Chargement du contenu du fichier en documents LangChain
        docs = loader.load()
        documents.extend(docs)

    # D√©coupage des documents en chunks de 1000 caract√®res avec 100 de recouvrement
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    chunks = splitter.split_documents(documents)

    # Initialisation du mod√®le d‚Äôembedding OpenAI
    embeddings = OpenAIEmbeddings(openai_api_key=api_key)

    # Indexation vectorielle avec FAISS (recherche rapide par similarit√©)
    vectordb = FAISS.from_documents(chunks, embedding=embeddings)

    return vectordb  # On retourne la base vectorielle pr√™te √† √™tre interrog√©e